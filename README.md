# ğŸ’° ETL Bitcoin - Airflow, PostgreSQL & Docker

## ğŸ’¡ Ideia do Projeto

O projeto consiste em um **pipeline ETL (Extract, Transform, Load)** para coletar dados da **API de Bitcoin**, processÃ¡-los com **Pandas** e armazenÃ¡-los em um **banco de dados PostgreSQL**.

Toda a orquestraÃ§Ã£o Ã© feita com o **Apache Airflow**, garantindo que as DAGs sejam executadas **a cada 15 minutos** para manter os dados sempre atualizados.

O ambiente Ã© **totalmente containerizado com Docker**, permitindo fÃ¡cil reprodutibilidade e implantaÃ§Ã£o em qualquer ambiente.

---

## ğŸ§­ Fluxo de Dados

```mermaid
flowchart LR
    A[ğŸŒ API de Bitcoin<br>Fonte de Dados] --> B[âš™ï¸ Airflow<br>Orquestra o ETL]
    B --> C[ğŸ“Š Pandas<br>Transforma e Limpa os Dados]
    C --> D[(ğŸ˜ PostgreSQL<br>Armazena Dados Processados)]
```

O Airflow agenda as execuÃ§Ãµes de ETL, que fazem:
1. **ExtraÃ§Ã£o:** Consome os dados da API de Bitcoin.
2. **TransformaÃ§Ã£o:** Limpa, valida e estrutura os dados com Pandas.
3. **Carga:** Insere ou atualiza os registros no banco PostgreSQL.

---

## ğŸ› ï¸ Tecnologias Utilizadas

![Python](https://img.shields.io/badge/Python-3.11-blue)
![Docker](https://img.shields.io/badge/Docker-Engine-blue)
![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.8-green)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue)
![Pandas](https://img.shields.io/badge/Pandas-2.1.4-green)
![Requests](https://img.shields.io/badge/Requests-HTTP-blue)

---

## ğŸ—‚ï¸ Estrutura do Projeto

```bash
â”œâ”€â”€ dags/                                 # ContÃ©m os pipelines (DAGs) do Airflow
â”‚   â””â”€â”€ etl/
â”‚       â”œâ”€â”€ dags/                         # Arquivos de DAGs principais
â”‚       â””â”€â”€ src/                          # CÃ³digo-fonte do ETL
â”‚           â”œâ”€â”€ data/                     # DiretÃ³rio principal de dados e lÃ³gica de ETL
â”‚           â”‚   â”œâ”€â”€ bitcoin_quotes/      # ExtraÃ§Ã£o de cotaÃ§Ãµes de Bitcoin
â”‚           â”‚   â”œâ”€â”€ databases_connection/ # Gerenciamento de conexÃµes com bancos SQL
â”‚           â”‚   â”œâ”€â”€ bitcoin_data/          # TransformaÃ§Ãµes e manipulaÃ§Ã£o de dados
â”‚           â”‚   â”œâ”€â”€ utils/                # FunÃ§Ãµes auxiliares reutilizÃ¡veis
â”‚           â”‚   â””â”€â”€ worker/               # Workers especÃ­ficos por moeda ou tipo de dado
â”œâ”€â”€ logs/                                 # Logs do Airflow
â”œâ”€â”€ plugins/                              # Plugins personalizados (operadores, sensores, hooks, etc.)
```

---

## âš™ï¸ Como Executar o Projeto

### ğŸ§° PrÃ©-requisitos

Certifique-se de ter instalado:

- [Docker](https://docs.docker.com/get-docker/)  
- [Docker Compose](https://docs.docker.com/compose/)

---

### 1ï¸âƒ£ Clone o repositÃ³rio
```bash
git clone https://github.com/joaobarreto27/etl_bitcoin_jd
cd etl-bitcoin-airflow
```

---

### 2ï¸âƒ£ Configure as variÃ¡veis de ambiente

Crie um arquivo `.env` na raiz do projeto com o conteÃºdo:

```env
# Banco de Dados
POSTGRES_DB=bitcoin_db
POSTGRES_USER=admin
POSTGRES_PASSWORD=admin
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
```

---

### 3ï¸âƒ£ Inicie o ambiente Docker

```bash
# 1ï¸âƒ£ Buildar tudo (sem cache):
docker-compose build --no-cache

# 2ï¸âƒ£ Subit somente o cotainer init:
docker-compose up airflow-init

# 3ï¸âƒ£ Subir todo o restante do ambiente ambiente:
docker-compose up -d
```

Os comandos acima irÃ¡ subir:
- O **Airflow Scheduler**
- O **Airflow Webserver**
- O **PostgreSQL**
- O **Airflow Worker**

A interface do Airflow estarÃ¡ disponÃ­vel em:
ğŸ‘‰ [http://localhost:8080](http://localhost:8080)

---

## ğŸ’» Executando Localmente (sem Docker) com Poetry
```bash
# 1ï¸âƒ£ Instale o Poetry se ainda nÃ£o tiver
curl -sSL https://install.python-poetry.org | python3 -

# 2ï¸âƒ£ Instale dependÃªncias do projeto via Poetry
poetry install

# 3ï¸âƒ£ Ative o ambiente do Poetry
poetry shell

# 4ï¸âƒ£ Execute um pipeline de exemplo
cd dags/etl/
python src/worker/quotes_btc/etl_quotes_btc_daily_event.py
```

## ğŸ•“ Agendamento das DAGs

A DAG Ã© configurada para rodar **a cada 15 minutos**, conforme exemplo abaixo:

```python
from datetime import timedelta

schedule_interval = "*/15 * * * *"  # Executa a cada 15 minutos
```

---

## ğŸ§© Pipeline ETL

**1. Extract:**  
Busca os dados da API de Bitcoin.  
Exemplo de retorno:
```json
{
  "data": {
    "base": "BTC",
    "currency": "USD",
    "amount": "67231.54"
  }
}
```

**2. Transform:**  
Converte o JSON em DataFrame Pandas, valida valores nulos, e formata timestamps.

**3. Load:**  
Insere no PostgreSQL com controle de duplicidade e schema definido.

---

## ğŸ’¾ Banco de Dados

O banco PostgreSQL Ã© inicializado automaticamente via Docker Compose.  
VocÃª pode acessar o banco usando:

```bash
docker exec -it postgres psql -U admin -d bitcoin_db
```

### Estrutura da Tabela

| Coluna       | Tipo       | DescriÃ§Ã£o                    |
|---------------|------------|-------------------------------|
| id            | SERIAL PK  | Identificador Ãºnico           |
| base_currency | TEXT       | Moeda base (ex: BTC)          |
| target_currency | TEXT     | Moeda de conversÃ£o (ex: USD)  |
| amount        | NUMERIC    | Valor da cotaÃ§Ã£o              |
| timestamp     | TIMESTAMP  | Data/hora da coleta           |

---

## ğŸ” Logs e Monitoramento

Os logs das DAGs podem ser acessados diretamente na interface do **Airflow**, em:
`Admin â†’ DAGs â†’ bitcoin_dag â†’ Log`

---

## ğŸ’¾ Banco de Dados

O projeto utiliza **PostgreSQL** em produÃ§Ã£o, mas tambÃ©m suporta **SQLite** para testes locais.

Exemplo de configuraÃ§Ã£o no cÃ³digo:

```python
USE_SQLITE = True

connection = ConnectionDatabaseSpark(
    sgbd_name="sqlite" if USE_SQLITE else "postgresql",
    environment="prd" if USE_SQLITE else "prd",
    db_name="etl_bitcoin_jd",
)
```

---

## ğŸ§  Boas PrÃ¡ticas Adotadas

- Uso de **Pandas** para limpeza e transformaÃ§Ã£o eficiente.  
- **ValidaÃ§Ã£o de dados** antes da carga no banco (schema + tipos).  
- **Atomicidade** nas transaÃ§Ãµes com SQLAlchemy.  
- **Docker Compose** para reprodutibilidade total do ambiente.  
- **Versionamento de dependÃªncias** no `requirements.txt`.  
- **Isolamento modular**: cada etapa (Extract, Transform, Load) em seu prÃ³prio mÃ³dulo.

---

## ğŸ§‘â€ğŸ’» Autor

ğŸ‘¤ **JoÃ£o Vitor**  
ğŸ’¼ Engenheiro de Dados  
ğŸ“§ [joaovitor@email.com](mailto:joao.vito1951@gmail.com)  
ğŸ™ [GitHub](https://github.com/joaobarreto27)  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/jo%C3%A3o-vitor-barreto-495a6a222/)

---

## ğŸ“š ReferÃªncias

- [Apache Airflow Docs](https://airflow.apache.org/docs/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Coinbase API Reference](https://developers.coinbase.com/api/v2)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [Docker Docs](https://docs.docker.com/)

---
